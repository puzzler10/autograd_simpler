{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing autograd from scratch\n",
    "\n",
    "> This will have both code and documentation in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.core.debugger import set_trace\n",
    "from IPython.display import Markdown as md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import autograd \n",
    "path_assets = './assets/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequsites "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* you know what backpropagation is and roughly how it works\n",
    "* you know what autodifferentiation is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are we trying to find?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to have a function we want to differentiate. This function will take in some number of variables: one, two, three... however many. If our function is called $f$, we are looking to find $df/dv$, where $v$ is one of these variables.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a function in two variables: x and y. $$f = \\log(x^2) + y^2 + xy$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x,y):    return np.log(x**2) + y**2 + x*y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim is to create a function `grad` that takes `f` as an input and returns a function with its gradient wrt a variable. So for our function above, `grad` can give us the answer of either $df/dx$ or $df/dy$, depending on what variable we tell it to differentiate with respect to. Let's define `grad`, giving it dummy values for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we choose this variable? We don't do it by name, telling the function either 'x' or 'y'. Rather, we give it a number (call it `argnum`), and say, \"Differentiate `f` with respect to the `argnum`th input.\" `f` has an argument list, and `argnum` is the index of this variable list, which corresponds to the variable we are interested in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`grad` has a nested structure: it returns a function `gradfun` that in turn returns the gradient. Notice the use of `*args` and `**kwargs` below. This is needed in the inner function `gradfun` because a) it doesnt' know how many arguments `f` takes; b) you can choose to call those arguments either with or without keywords. If you call them without keywords, the arguments are stored in `*args`, and if you call them with keywords, the arguments are stores in `**kwargs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args (1, 2)\n",
      "Kwargs {}\n",
      "dfdx 137.37736658799992\n",
      "Args ()\n",
      "Kwargs {'x': 13, 'y': 4}\n",
      "dfdy 21\n"
     ]
    }
   ],
   "source": [
    "def grad(f, argnum = 0): \n",
    "    \"\"\"Returns a function that finds the gradient\"\"\"\n",
    "    def gradfun(*args, **kwargs):\n",
    "        \"\"\"Returns the actual gradient \"\"\"\n",
    "        #set_trace()\n",
    "        print (\"Args\", args)\n",
    "        print (\"Kwargs\", kwargs)\n",
    "        # Dummy values. Returns correct gradient only for our function f above. \n",
    "        # Use these values until we calculate the true ones using autodiff. \n",
    "        #### remove this code once true code written\n",
    "        if   argnum == 0: return 2*x * np.log(x**2) + y  # df/dx\n",
    "        elif argnum == 1: return 2*y + x                 # df/dy\n",
    "        #### \n",
    "        # true autograd code goes here \n",
    "        ####\n",
    "    return gradfun\n",
    "# example usage\n",
    "dfdx = grad(f, argnum = 0)\n",
    "dfdy = grad(f, argnum = 1)\n",
    "print(\"dfdx\", dfdx(1,2))      # call gradient w/out keywords, values go into *args    in gradfun \n",
    "print(\"dfdy\", dfdy(x=13,y=4)) # call gradient with  keywords, values go into **kwargs in gradfun "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's our goal. We are trying to build this `grad` function properly, following the structure defined above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a computation graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say you had some expression, like $ (4 \\times 5) + 2 - 4$. I'm sure you know the answer to this, but how would a computer work it out? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may remember the order of operations used to work out these expressions; I learnt the acronym [BODMAS](https://www.mathsisfun.com/operation-order-bodmas.html) to remember these. Python has its own order of operations too, governed by the hierarchy of [operator precedence](https://docs.python.org/3/reference/expressions.html#operator-precedence). Python breaks down an expression and executes it in this order. You can view the breakdown of an expression as a computation graph, where each node is an action (x3, +5, log) and where the graph doesn't have any loops (it is a DAG). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computation graph is very important to us. It is used in the backpropagation algorithm. The idea is that becuase each node of the graph is a simple operation (like +, x, log), working out its derivative is also easy. The gradient at a point on the computation graph is called the local gradient. Start at the head node and combine local gradients together until you reach a leaf node. Then this leaf node has the answer you seek. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a way to build the computation graph. This is quite a clever idea actually. Typically your functions are made up of operators from the numpy package. What we will do is create a copy of the numpy package and use that copy instead of the original numpy package. The copy works exactly like the original does, except that when it executes functions, it builds a computation graph for us. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to show this is by an example. Let's say we had the following function: \n",
    "$$ logistic(z) = \\frac{1}{1 + \\exp(-z)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would implement this in code like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(z): return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Numpy typically overwrites common operators, meaning it replaces $+, \\times, / $ with its own numpy equivalents `np.add`, `np.multiply`, `np.divide` and so on. It does this defining the methods  `__add__`,`__mult__`,`__div__` in the `numpy.ndarray` class. So the effect of this is that `logistic(z)` gets transformed into something like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic2(z): return np.reciprocal(np.add(1, np.exp(np.negative(z))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the order of operations, or in other words the order that Python breaks down this expression. This has the same effect as constructing a number of intermediate variables, one after each operation. Call the intermediate variables $t_1, t_2, t_3...$, the input to the function $z$, and the final value $y$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic3(z): \n",
    "    t1 = np.negative(z) \n",
    "    t2 = np.exp(t1)\n",
    "    t3 = np.add(1, t2)\n",
    "    y = np.reciprocal(t3) \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to turn `logistic2` into a computation graph, with nodes and links between them. Here's what this graph looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/node_tree.svg\" width=\"240\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will eventually define a class called `Node` and each node in the graph will be a member of this class. We define the links between nodes with the 'parents' attribute. Leaf nodes do not have parents. The leaf nodes above are $1$ and $z$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a representation of the computational graphs, now using Nodes. The numbers in `value` indicate the value of that intermediate variable. The function was given $z=1.5$ as an input and outputs $y=0.818$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/node_tree.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm we get the same answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.818"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(logistic3(1.5),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructing the Node class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can construct a first version of the Node class. For each Node, we need at least `value`, a function (`fun`) and `parents`. Let's create an tuple called `recipe` that we store `fun` and `value` in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also will create a function called `initialise_root` that starts off the graph. A root of the tree doesn't have any parents, its function is just the identity function, and it has no value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"A node in a computation graph.\"\"\"\n",
    "    def __init__(self, value, fun, parents):\n",
    "        self.parents = parents\n",
    "        self.recipe = (fun, value)\n",
    "\n",
    "    def initialize_root(self):\n",
    "        self.parents = []\n",
    "        self.recipe = (lambda x: x, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the Node class, we could manually build a computational graph if we wanted to. (We don't create a Node for $1$ or other scalars, just intermediate variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.818\n"
     ]
    }
   ],
   "source": [
    "val_z = 1.5 \n",
    "z = Node(val_z, None, [])\n",
    "val_t1 = np.negative(val_z)\n",
    "t1 = Node(val_t1,np.negative, [z])\n",
    "val_t2 = np.exp(val_t1)\n",
    "t2 = Node(val_t2, np.exp, [t1])\n",
    "val_t3 = np.add(val_t2, 1)\n",
    "t3 = Node(val_t3, np.add, [t2])\n",
    "val_y = np.reciprocal(val_t3)\n",
    "y = Node(val_y, np.reciprocal, [t3])\n",
    "print(round(y.recipe[1],3)) # same answer as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the computational graph this way is both manual and clunky. Time now to build it automatically. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a new version of Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be great if numpy created a node for each intermediate variable and added it to our graph. But it won't do that. So it's time to make our own version of Numpy so we can trace the flow of execution and create our computation graph. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll delete the reference to our current numpy and import it under a new name, `onp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:     del np\n",
    "except:  pass \n",
    "import numpy as onp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this section, I'll refer to the original numpy as `onp`, and the version we are building as `anp`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a few things we have to do: \n",
    "\n",
    "* create new versions of `onp` functions. For example, we need to create a function `anp.add` that does everything `onp.add` does, but it also adds a node to the computation graph when called. Same with `anp.multiply`, `anp.divide` etc. \n",
    "* overload operators like $+, \\times, /$ so that they use the `anp` versions: `anp.add`, `anp.multiply`, `anp.divide`, by defining functions like `__add__`, `__mul__`, `__div__`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primitives "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boxes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping Nodes in Boxes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boxes are used to indicate the variable you are differentiating with respect to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making VJPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backwards pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the following resources to put this document together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture slides by Roger Grosse: https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
